---
layout: post
title: "Deep Double Descent: Where Bigger Models and More Data Hurt"
tags: ["double descent","learning theory"]
date: 2019-12-09 11:57:21 +0900
---

統計的学習理論の教えるところによると，モデルサイズを大きくしすぎると過適合し汎化誤差が増大する．しかし近年，ある閾値を超えるとモデルサイズの増大とともに汎化誤差が減少する現象（double descent）が報告されている．この論文では，深層学習モデルにおけるdouble descent現象を報告．モデルサイズだけでなく学習エポック数についてもdouble descentが見られることを報告．また，Effective Model Complexityを導入し，その原因について考察．

## 基本情報
### 会議・論文誌

### 論文リンク
https://arxiv.org/abs/1912.02292

### 著者・所属
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever

## 新規性

## 手法

## 結果

## 議論・コメント

## 関連文献

- Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018

## Original issue and comments

https://github.com/yoshum/daily-research-news/issues/142
