---
layout: post
title: "On the Relationship between Self-Attention and Convolutional Layers"
tags: ["arxiv","attention"]
date: 2019-11-13 22:31:01 +0900
---

Self-attentionが畳込み演算を表現できることを構成的な証明を与えた．また，実際にattention-only architectureで実験すると，最初の数層は理論で与えた構成と類似したグリッド上のパターンを学習することがわかった．

## 基本情報
### 会議・論文誌

### 論文リンク
https://arxiv.org/abs/1911.03584

### 著者・所属
Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi (EPFL)

### 投稿日
2019/11/08 (arXiv)

## 新規性

- Self-attentionが畳込み演算を表現できることを構成的な証明を与えた
- Attention-only architectureの最初の数層は，理論で与えた構成と類似したグリッド上のパターンを学習することを実験的に示した

## 手法
![image](https://user-images.githubusercontent.com/17794644/68767970-009d6500-0665-11ea-80f3-fe7392721fa1.png)

## 結果
![image](https://user-images.githubusercontent.com/17794644/68767855-cdf36c80-0664-11ea-9354-9848010dfc8b.png)

## 議論・コメント

## 関連文献


## Original issue and comments

https://github.com/yoshum/daily-research-news/issues/91
